{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facemesh.faceDetector import FaceDetector, cv2\n",
    "from facemesh.utils import eye_aspect_ratio\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def draw_polyline(img, l):\n",
    "    cv2.polylines(img, np.array([l], dtype=np.int32), True, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "## Landmarks keypoints:\n",
    "## https://github.com/tensorflow/tfjs-models/blob/838611c02f51159afdd77469ce67f0e26b7bbb23/face-landmarks-detection/src/mediapipe-facemesh/keypoints.ts\n",
    "lipsUpperOuter = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter = [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "lipsUpperInner = [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
    "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ] \n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "pTime = 0\n",
    "detector = FaceDetector()\n",
    "\n",
    "# counters:\n",
    "blinkCount = 0\n",
    "\n",
    "while True:\n",
    "    left_eye_lm = []\n",
    "    right_eye_lm = []\n",
    "    lipsUpperOuter_lm, lipsLowerOuter_lm, lipsUpperInner_lm, lipsLowerInner_lm = [],[],[],[]\n",
    "    success, img = cap.read()\n",
    "    if success:\n",
    "        l = detector.facemeshing(img, draw = False)\n",
    "        if l:\n",
    "            # Get left and right eye\n",
    "            for lm_id in LEFT_EYE:\n",
    "                left_eye_lm.append(l[0][lm_id])\n",
    "            \n",
    "            for lm_id in RIGHT_EYE:\n",
    "                right_eye_lm.append(l[0][lm_id])\n",
    "            # Get lips: \n",
    "            for lm_id in lipsUpperOuter:\n",
    "                lipsUpperOuter_lm.append(l[0][lm_id])\n",
    "            \n",
    "            for lm_id in lipsLowerOuter:\n",
    "                lipsLowerOuter_lm.append(l[0][lm_id])\n",
    "            for lm_id in lipsUpperInner:\n",
    "                lipsUpperInner_lm.append(l[0][lm_id])\n",
    "            \n",
    "            for lm_id in lipsLowerInner:\n",
    "                lipsLowerInner_lm.append(l[0][lm_id])\n",
    "            \n",
    "            # getting leftEAR from eye_aspect_ratio.\n",
    "            # eye_aspect_ratio computes distances betwen the two sets of vertical eye landmarks coordinates.\n",
    "\n",
    "            draw_polyline(img,left_eye_lm )\n",
    "            draw_polyline(img,right_eye_lm )\n",
    "            draw_polyline(img,lipsUpperOuter_lm + lipsLowerOuter_lm[::-1])\n",
    "            draw_polyline(img,lipsUpperInner_lm + lipsLowerInner_lm[::-1])\n",
    "        else:\n",
    "            pass\n",
    "    cTime = time.time()\n",
    "    fps = 1 / (cTime-pTime)\n",
    "    pTime = cTime\n",
    "    \n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,50), cv2.FONT_HERSHEY_PLAIN,3, (0,255, 255), 2)\n",
    "    cv2.imshow('face',img)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        cap.release() \n",
    "        break  \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input vector should be 1-D.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\israe\\Codes\\attention-monitor\\attention\\notebooks\\analyze.ipynb Celda 2\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/israe/Codes/attention-monitor/attention/notebooks/analyze.ipynb#W1sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m leftEye \u001b[39m=\u001b[39m LEFT_EYE\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/israe/Codes/attention-monitor/attention/notebooks/analyze.ipynb#W1sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m rightEye \u001b[39m=\u001b[39m RIGHT_EYE\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/israe/Codes/attention-monitor/attention/notebooks/analyze.ipynb#W1sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m leftEAR \u001b[39m=\u001b[39m eye_aspect_ratio(leftEye)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/israe/Codes/attention-monitor/attention/notebooks/analyze.ipynb#W1sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m rightEAR \u001b[39m=\u001b[39m eye_aspect_ratio(rightEye)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/israe/Codes/attention-monitor/attention/notebooks/analyze.ipynb#W1sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m ear \u001b[39m=\u001b[39m (leftEAR \u001b[39m+\u001b[39m rightEAR) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\israe\\Codes\\attention-monitor\\attention\\notebooks\\modules\\utils.py:9\u001b[0m, in \u001b[0;36meye_aspect_ratio\u001b[1;34m(eye)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meye_aspect_ratio\u001b[39m(eye):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# compute the euclidean distances between the two sets of\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# vertical eye landmarks (x, y)-coordinates\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     A \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39;49meuclidean(eye[\u001b[39m1\u001b[39;49m], eye[\u001b[39m5\u001b[39;49m])\n\u001b[0;32m     10\u001b[0m     B \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39meuclidean(eye[\u001b[39m2\u001b[39m], eye[\u001b[39m4\u001b[39m])\n\u001b[0;32m     11\u001b[0m     \u001b[39m# compute the euclidean distance between the horizontal\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39m# eye landmark (x, y)-coordinates\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\spatial\\distance.py:518\u001b[0m, in \u001b[0;36meuclidean\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meuclidean\u001b[39m(u, v, w\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    483\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[39m    Computes the Euclidean distance between two 1-D arrays.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m     \u001b[39mreturn\u001b[39;00m minkowski(u, v, p\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, w\u001b[39m=\u001b[39;49mw)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\spatial\\distance.py:461\u001b[0m, in \u001b[0;36mminkowski\u001b[1;34m(u, v, p, w)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminkowski\u001b[39m(u, v, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, w\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    412\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[39m    Compute the Minkowski distance between two 1-D arrays.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    459\u001b[0m \n\u001b[0;32m    460\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     u \u001b[39m=\u001b[39m _validate_vector(u)\n\u001b[0;32m    462\u001b[0m     v \u001b[39m=\u001b[39m _validate_vector(v)\n\u001b[0;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\spatial\\distance.py:301\u001b[0m, in \u001b[0;36m_validate_vector\u001b[1;34m(u, dtype)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mif\u001b[39;00m u\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m u\n\u001b[1;32m--> 301\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput vector should be 1-D.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Input vector should be 1-D."
     ]
    }
   ],
   "source": [
    "from modules.faceDetector import FaceDetector, cv2\n",
    "from modules.utils import eye_aspect_ratio, mouth_aspect_ratio, rec_to_roi_box, crop_img\n",
    "from modules.facepose import Facepose\n",
    "from imutils import face_utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import dlib\n",
    "\n",
    "\n",
    "def draw_polyline(img, l):\n",
    "    cv2.polylines(img, np.array([l], dtype=np.int32), True, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "def showFPS(img, pTime):\n",
    "    cTime = time.time()\n",
    "    fps = 1 / (cTime-pTime)\n",
    "    pTime = cTime\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,50), cv2.FONT_HERSHEY_PLAIN, 3, (0,255, 255), 2)\n",
    "    return pTime, cTime\n",
    "\n",
    "def averageFPS(img, counter, pTime, prev_fps, interval = 1 ):\n",
    "    counter +=1\n",
    "    cTime = time.time()\n",
    "    if cTime-pTime > interval:\n",
    "        fps = int(counter/(cTime-pTime))\n",
    "        counter = 0\n",
    "        pTime = cTime\n",
    "    else:\n",
    "        fps = prev_fps\n",
    "    #cv2.putText(img, f'FPS: {int(fps)}', (80,50), cv2.FONT_HERSHEY_PLAIN, 3, (0,255, 255), 2)\n",
    "    return fps, pTime, counter\n",
    "\n",
    "def euclaidean_distance(p1, p2):\n",
    "    x,y = p1\n",
    "    x1, y1 = p2\n",
    "    distance = math.sqrt((x1-x)**2 + (y1-1)**2)\n",
    "    return distance\n",
    "    \n",
    "def blinking_ratio(img, landmarks, right_indices, left_indices, draw = False):\n",
    "    # right eye\n",
    "    # horizontal line\n",
    "    rh_right = landmarks[right_indices[0]]\n",
    "    rh_left = landmarks[right_indices[8]]\n",
    "    # vertical line\n",
    "    rv_top = landmarks[right_indices[12]]\n",
    "    rv_bottom = landmarks[right_indices[4]]\n",
    "\n",
    "    if draw:\n",
    "        cv2.line(img, rh_right,rh_left, (255,128,0), 2)\n",
    "        cv2.line(img, rv_top,rv_bottom, (0,0,0), 2)\n",
    "\n",
    "    # left eye\n",
    "    # horizontal line\n",
    "    lh_right = landmarks[right_indices[0]]\n",
    "    lh_left = landmarks[right_indices[8]]\n",
    "    # vertical line\n",
    "    lv_top = landmarks[right_indices[12]]\n",
    "    lv_bottom = landmarks[right_indices[4]]\n",
    "    pass\n",
    "\n",
    "\n",
    "## Landmarks keypoints:\n",
    "## https://github.com/tensorflow/tfjs-models/blob/838611c02f51159afdd77469ce67f0e26b7bbb23/face-landmarks-detection/src/mediapipe-facemesh/keypoints.ts\n",
    "lipsUpperOuter = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter = [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "lipsUpperInner = [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
    "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ] \n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "\n",
    "#hopenet_dir = '../../model/hopenet_robust_alpha1.pkl'\n",
    "model_dir = \"../../model/shape_predictor_68_face_landmarks.dat\"\n",
    "faceDetector = FaceDetector()\n",
    "#facepose = Facepose(hopenet_dir)\n",
    "predictor = dlib.shape_predictor(model_dir)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# counters:\n",
    "blinkCount = 0\n",
    "yawnCount = 0\n",
    "fps, pTime, frame_counter = 0,0,0\n",
    "\n",
    "lostFocusCount = 0\n",
    "lostFocusDuration = 0\n",
    "focusTimer = None\n",
    "# states\n",
    "yawning = False\n",
    "eyeClosed = False\n",
    "lostFocus = False\n",
    "\n",
    "while True:\n",
    "    left_eye_lm = []\n",
    "    right_eye_lm = []\n",
    "    lipsUpperOuter_lm, lipsLowerOuter_lm, lipsUpperInner_lm, lipsLowerInner_lm = [],[],[],[]\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if success: ## si captura imagen.\n",
    "        frame_gray = cv2.cvtColor(frame.copy(), cv2.COLOR_BGR2GRAY)\n",
    "        faces = faceDetector.findFaces(frame, d_square=False)\n",
    "        faceDetector.head_tilting(frame)\n",
    "\n",
    "        #print(bb[0][0]) ## one face (4 coordinates) and rate.\n",
    "        if not len(faces) == 0: \n",
    "            for rostro in faces[0][0]:\n",
    "                x, y, w, h = faces[0][0] #first point, bridging point\n",
    "                x1, y1 = x+w, y+h #bottom right point\n",
    "                cv2.rectangle(frame,(x, y),(x1, y1), (0,255,0),1 )\n",
    "                \n",
    "\n",
    "                leftEye = LEFT_EYE\n",
    "                rightEye = RIGHT_EYE\n",
    "                \n",
    "                \n",
    "                blinking_ratio(frame, rightEye, leftEye)\n",
    "                \n",
    "                '''dlib too slow\n",
    "\n",
    "                rect = dlib.rectangle(left= x, top= y, right= x1, bottom= y1 ) # generar rect√°ngulo\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                shape = predictor(frame_gray, rect) \n",
    "                shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "                leftEye = shape[36:42]\n",
    "                rightEye = shape[42:48]\n",
    "                leftEAR = eye_aspect_ratio(leftEye)\n",
    "                rightEAR = eye_aspect_ratio(rightEye)\n",
    "                ear = (leftEAR + rightEAR) / 2.0\n",
    "                mar = mouth_aspect_ratio(shape[60:69])\n",
    "\n",
    "                if ear < 0.15:\n",
    "                    eyeClosed = True\n",
    "                if ear > 0.15 and eyeClosed:\n",
    "                    blinkCount += 1\n",
    "                    eyeClosed = False\n",
    "\n",
    "                if mar > 0.4:\n",
    "                    yawning = True\n",
    "                if mar < 0.2 and yawning:\n",
    "                    yawnCount += 1\n",
    "                    yawning = False\n",
    "\n",
    "                roi_box, center_x, center_y = rec_to_roi_box(rect)\n",
    "                roi_img = crop_img(frame, roi_box)\n",
    "                img = Image.fromarray(roi_img)\n",
    "                '''\n",
    "                \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # FPS \n",
    "    fps, pTime, frame_counter = averageFPS(frame,frame_counter, pTime, fps)\n",
    "    #pTime, cTime = showFPS(frame, pTime)\n",
    "    cv2.putText(frame, f'FPS: {int(fps)}', (80,50), cv2.FONT_HERSHEY_PLAIN, 3, (0,255, 255), 2)\n",
    "\n",
    "    cv2.imshow('face',frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release() \n",
    "        break  \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.faceDetector import FaceDetector, cv2\n",
    "from modules.units.mouthUnit import MouthUnit\n",
    "import time\n",
    "\n",
    "def averageFPS(img, counter, pTime, prev_fps, interval = 1 ):\n",
    "    counter +=1\n",
    "    cTime = time.time()\n",
    "    if cTime-pTime > interval:\n",
    "        fps = int(counter/(cTime-pTime))\n",
    "        counter = 0\n",
    "        pTime = cTime\n",
    "    else:\n",
    "        fps = prev_fps\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (80,50), cv2.FONT_HERSHEY_PLAIN, 3, (0,255, 255), 2)\n",
    "    return fps, pTime, counter\n",
    "\n",
    "facer = FaceDetector()\n",
    "mouth = MouthUnit()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "pTime = 0\n",
    "frame_counter= 0\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame_shape = frame.shape[:2]\n",
    "        #facer.findFaces(frame, d_square=True)\n",
    "        results = facer.get_results(frame)\n",
    "        #facer.facemeshing(results,frame, draw=False)\n",
    "        mesh_list = facer.landmarksDetection(frame, results, frame_shape, draw=False)\n",
    "        mouth.mouth_aspect_ratio(frame, mesh_list, draw=True)\n",
    "\n",
    "        # FPS     \n",
    "    #pTime, cTime = showFPS(frame, pTime)\n",
    "    fps, pTime, frame_counter = averageFPS(frame, frame_counter, pTime, fps)\n",
    "\n",
    "    #cv2.putText(frame, f'FPS: {int(fps)}', (20,50), cv2.FONT_HERSHEY_PLAIN, 3, (0,255, 255), 2)\n",
    "    cv2.imshow('face',frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release() \n",
    "        break  \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('attention_monitor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b11c4971c94b41e73dae8cae6ab4c5412a34c5a1f11c11f3e236bcae8092626a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
