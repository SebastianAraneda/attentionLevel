{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n",
      "face detected\n"
     ]
    }
   ],
   "source": [
    "from facemesh.faceDetector import FaceDetector, cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def draw_polyline(img, l):\n",
    "    cv2.polylines(img, np.array([l], dtype=np.int32), True, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "## Landmarks keypoints:\n",
    "## https://github.com/tensorflow/tfjs-models/blob/838611c02f51159afdd77469ce67f0e26b7bbb23/face-landmarks-detection/src/mediapipe-facemesh/keypoints.ts\n",
    "lipsUpperOuter = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter = [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "lipsUpperInner = [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
    "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ] \n",
    "LEFT_IRIS = [474,475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "pTime = 0\n",
    "detector = FaceDetector()\n",
    "\n",
    "\n",
    "while True:\n",
    "    left_eye_lm = []\n",
    "    right_eye_lm = []\n",
    "    lipsUpperOuter_lm, lipsLowerOuter_lm, lipsUpperInner_lm, lipsLowerInner_lm = [],[],[],[]\n",
    "    success, img = cap.read()\n",
    "    if success:\n",
    "        l = detector.facemeshing(img, draw = False)\n",
    "        if l:\n",
    "            print(\"face detected\")\n",
    "            # Get left and right eye\n",
    "            for lm_id in LEFT_EYE:\n",
    "                left_eye_lm.append(l[0][lm_id])\n",
    "            \n",
    "            for lm_id in RIGHT_EYE:\n",
    "                right_eye_lm.append(l[0][lm_id])\n",
    "            # Get lips: \n",
    "            for lm_id in lipsUpperOuter:\n",
    "                lipsUpperOuter_lm.append(l[0][lm_id])\n",
    "            \n",
    "            for lm_id in lipsLowerOuter:\n",
    "                lipsLowerOuter_lm.append(l[0][lm_id])\n",
    "            for lm_id in lipsUpperInner:\n",
    "                lipsUpperInner_lm.append(l[0][lm_id])\n",
    "            \n",
    "            for lm_id in lipsLowerInner:\n",
    "                lipsLowerInner_lm.append(l[0][lm_id])\n",
    "            \n",
    "            draw_polyline(img,left_eye_lm )\n",
    "            draw_polyline(img,right_eye_lm )\n",
    "            draw_polyline(img,lipsUpperOuter_lm + lipsLowerOuter_lm[::-1])\n",
    "            draw_polyline(img,lipsUpperInner_lm + lipsLowerInner_lm[::-1])\n",
    "        else:\n",
    "            print(\"no face detected.\")\n",
    "    cTime = time.time()\n",
    "    fps = 1 / (cTime-pTime)\n",
    "    pTime = cTime\n",
    "    \n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,50), cv2.FONT_HERSHEY_PLAIN,3, (0,255, 255), 2)\n",
    "    cv2.imshow('face',img)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        cap.release() \n",
    "        break  \n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class FaceDetector():\n",
    "    def __init__(self, minDetConf = 0.5):\n",
    "        self.mpFaceDetection = mp.solutions.face_detection\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.faceDetection = self.mpFaceDetection.FaceDetection(minDetConf)\n",
    "        self.mpFaceMesh = mp.solutions.face_mesh\n",
    "        self.mpFaceMeshConnections = mp.solutions.face_mesh_connections\n",
    "        self.faceMesh = self.mpFaceMesh.FaceMesh(\n",
    "            max_num_faces=2,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5)\n",
    "        self.drawSpec = self.mpDraw.DrawingSpec(thickness=1, circle_radius=1)\n",
    "    \n",
    "    def findFaces(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = self.faceDetection.process(imgRGB)\n",
    "        bboxs = []\n",
    "        if results.detections:\n",
    "            for face_id, detection in enumerate(results.detections):\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, ic = img.shape\n",
    "                bbox =  int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                        int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                if draw: \n",
    "                    self.squareDraw(img, bbox)\n",
    "                    cv2.putText(img, \n",
    "                        f'{ int(detection.score[0]*100) }%', \n",
    "                        (bbox[0], bbox[1]-20),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, \n",
    "                        2, (255, 0, 255), 2)\n",
    "                bboxs.append([bbox, detection.score])\n",
    "        return bboxs\n",
    "\n",
    "    def squareDraw(self, img, bbox, l = 20, refctangle_t = 1, corner_t = 5):\n",
    "        x, y, w, h = bbox #first point, bridging point\n",
    "        x1, y1 = x+w, y+h #bottom right point\n",
    "        corner_color_y =  (0,225,128)\n",
    "        corner_color_x =  (0,225,128)\n",
    "        rectangle_color = (255,0,255)\n",
    "\n",
    "        cv2.rectangle(img, bbox, rectangle_color, refctangle_t)\n",
    "        cv2.line(img, (x,y), (x+l,y), corner_color_x, corner_t )\n",
    "        cv2.line(img, (x,y), (x,y+l), corner_color_y, corner_t )\n",
    "\n",
    "        cv2.line(img, (x1,y), (x1-l,y), corner_color_x, corner_t )\n",
    "        cv2.line(img, (x1,y), (x1,y+l), corner_color_y, corner_t )\n",
    "\n",
    "        cv2.line(img, (x,y1), (x+l,y1), corner_color_x, corner_t )\n",
    "        cv2.line(img, (x,y1), (x,y1-l), corner_color_y, corner_t )\n",
    "\n",
    "        cv2.line(img, (x1,y1), (x1-l,y1), corner_color_x, corner_t )\n",
    "        cv2.line(img, (x1,y1), (x1,y1-l), corner_color_y, corner_t )\n",
    "    \n",
    "    def facemeshing(self, img, draw = True, draw_square = True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = self.faceMesh.process(imgRGB)\n",
    "        faces = []\n",
    "        face = {}\n",
    "        if results.multi_face_landmarks:\n",
    "            for faceLms in results.multi_face_landmarks:\n",
    "                if draw:\n",
    "                    self.mpDraw.draw_landmarks(img, faceLms, self.mpFaceMesh.FACEMESH_LEFT_EYE,\n",
    "                                            self.drawSpec,self.drawSpec )\n",
    "\n",
    "                for id, lm in enumerate(faceLms.landmark):\n",
    "                    ih, iw, ic = img.shape\n",
    "                    x,y = int(lm.x*iw), int(lm.y*ih)\n",
    "                    #faces.append({id: [x,y]}) \n",
    "                    face[id]= [x,y]     ## adds every 478 landmark with its coordetates, for each face in list\n",
    "                faces.append(face)\n",
    "            return faces ## 1 or 2 faces in list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('attention_monitor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b11c4971c94b41e73dae8cae6ab4c5412a34c5a1f11c11f3e236bcae8092626a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
